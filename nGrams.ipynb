{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import kenlm\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import util\n",
    "import itertools\n",
    "from helpers import *\n",
    "import sys\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file\n",
    "with open('count_unigrams.txt','r') as f:\n",
    "    unigram_counts = f.readlines()\n",
    "\n",
    "with open('count_bigrams.txt', 'r') as fb:\n",
    "    bigram_counts = fb.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in a word and a count and splits it into the word and the integer.\n",
    "ex) parse_word_count('the\\t23135851162\\n') = (the, 23135851162)\n",
    "'''\n",
    "def parse_unigram_count(string):\n",
    "    return (string.split(\"\\t\")[0], int(string.split(\"\\t\")[1].split(\"\\n\")[0]))\n",
    "\n",
    "'''\n",
    "This function takes in 2 words and a count and splits it into the words and the integer.\n",
    "ex) parse_bigram_count ('important area\\t201828\\n') = (important, area, 201828)\n",
    "'''\n",
    "def parse_bigram_count(string):\n",
    "    return (string.split(\"\\t\")[0].split(\" \")[0], string.split(\"\\t\")[0].split(\" \")[1],int(string.split(\"\\t\")[1].split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A dictionary with (word: probability) pairs for 1/3 million most frequent English words \n",
    "unigram_dict = {parse_unigram_count(line)[0]:parse_unigram_count(line)[1] for line in unigram_counts}\n",
    "# A dictionary with (word + word: probability) apirs for 1/4 million most frequent English bigrams\n",
    "bigram_dict = {(parse_bigram_count(line)[0] + \" \" + parse_bigram_count(line)[1]):parse_bigram_count(line)[2] for line in bigram_counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider a unigram model, where the probability of seeing each word is independent of the words before and after it.\n",
    "P(W1:n)=Î k=1:nP(Wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a probability distribution dictionary unigram_probs from unigram_dict. But we will examine many, many \"words\" that are not truly words. Our unigram_probs dictionary must return a very small value in that case. (We do not want to simply return 0, because Jane Austen may use her own words (made up words, proper nouns, etc.) that are not in our data set. Instead, following Peter Norvig (CITE), we'll create a class that returns a probability given a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ngram_Prob(dict):\n",
    "    def __init__(self, data, N, fn):\n",
    "        for key,count in data.iteritems():\n",
    "            self[key] = count\n",
    "        self.N = float(N)\n",
    "        self.fn = fn\n",
    "    def __call__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]/self.N\n",
    "        else: \n",
    "            return self.fn(key, self.N)\n",
    "\n",
    "\n",
    "'''\n",
    "This function returns a probailtiy on unrecognized words. \n",
    "It makes it more unlikely for long unrecognized words to be used than short unrecognized words.\n",
    "The probability is inversely proportional to the length of the unrecognized word.\n",
    "'''\n",
    "def avoid_long_words(word, N):\n",
    "    return Decimal(10.)/(Decimal(N) * Decimal(10)**Decimal(len(word)))\n",
    "\n",
    "N = 1024908267229 ## Number of tokens in corpus\n",
    "p_unigram = ngram_Prob(unigram_dict, N, avoid_long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This memoizing function caches the results of previous calls to the segment\n",
    "function so that each results doesn't have to be recomputed.\n",
    "\n",
    "ex) segment(\"helloworld\") doesn't have to compute segment(\"lloworld\") except for once.\n",
    "\n",
    "The memoizing function helps segment only call itself n times, rather than 2^n times.\n",
    "'''\n",
    "def memoize(function):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo.keys():\n",
    "            memo[x] = function(x)\n",
    "        return memo[x]\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters\n",
    "    Corpus: The flattened text which we need to segment.\n",
    "Output\n",
    "    A list of words, separated according to our probability model.\n",
    "\n",
    "Ex) segment('helloworld') = ['hello','world']\n",
    "'''\n",
    "\n",
    "def tupleToString(tup):\n",
    "    string = ''\n",
    "    for i in range(len(tup)):\n",
    "        if i == len(tup)-1:\n",
    "            string += tup[i]\n",
    "        else:\n",
    "            string += (tup[i]+' ')\n",
    "    return string\n",
    "\n",
    "@memoize\n",
    "def segment(corpus):\n",
    "    if not corpus: \n",
    "        return []\n",
    "    candidates = tuple([first]+segment(remaining) for first,remaining in splits(corpus))\n",
    "    return max(candidates, key=Pwords)\n",
    "\n",
    "def splits(text, L=20):\n",
    "    '''\n",
    "    Return a list of all possible splits of the text, where length(first word)<=L.\n",
    "    '''\n",
    "#     return [(text[:i+1], text[i+1:]) for i in range(min(len(text), L))]\n",
    "#     return [(text[:i+1], text[i+1:]) for i in range(min(len(text), L))]\n",
    "    return [(text[:i+1], text[i+1:]) for i in range(len(text))]\n",
    "\n",
    "def Pwords(words):\n",
    "    '''\n",
    "    The Naive Bayes probability of a sequence of words.\n",
    "    '''\n",
    "    return product([Decimal(uProb(w)) for w in words])\n",
    "\n",
    "def uProb(word):\n",
    "    '''\n",
    "    Returns the unigram probability of a word by consulting unigram data.\n",
    "    '''\n",
    "    return p_unigram.__call__(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define new functions that use log probability to avoid overflow.\n",
    "from math import log10\n",
    "def log_prob(words):\n",
    "    return sum([log10(uProb(w)) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = alphaLowerNoSpace(\"senseAndSensibilityLast.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brett-This is my iterative version of the recursive function for unigrams only right now. It's working, and it's reasonably fast, but I think there might be a way to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns a dictionary of segments.\n",
    "def segmentString(string):\n",
    "    docs = splits(string)\n",
    "    docs.reverse()\n",
    "    docs.append((' ', string))\n",
    "    segments = {}\n",
    "    for first, remaining in docs:\n",
    "        if remaining == '':\n",
    "            segments[remaining] = [], Decimal(1.)\n",
    "        else:\n",
    "            rem = splits(remaining)\n",
    "            rem.reverse()\n",
    "#             segments[remaining] = max(([f] + segments[r] for f,r in rem), key=Pwords)\n",
    "            currentScore, bestScore, bestList = Decimal(0.), Decimal(0.), []\n",
    "            for f,r in rem:\n",
    "                currentList = [f] + segments[r][0]\n",
    "                currentScore = Pwords([f]) * segments[r][1]\n",
    "                if currentScore > bestScore: \n",
    "                    bestScore = currentScore\n",
    "                    bestList = currentList\n",
    "            segments[remaining] = bestList, bestScore\n",
    "    return segments[string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['after',\n",
       "  'a',\n",
       "  'proper',\n",
       "  'resistance',\n",
       "  'on',\n",
       "  'the',\n",
       "  'part',\n",
       "  'of',\n",
       "  'mrs',\n",
       "  'ferrars',\n",
       "  'just',\n",
       "  'so',\n",
       "  'violent',\n",
       "  'and',\n",
       "  'so',\n",
       "  'steady',\n",
       "  'as',\n",
       "  'to',\n",
       "  'preserve',\n",
       "  'her',\n",
       "  'from',\n",
       "  'that',\n",
       "  'reproach',\n",
       "  'which',\n",
       "  'she',\n",
       "  'always',\n",
       "  'seemed',\n",
       "  'fearful',\n",
       "  'of',\n",
       "  'incurring',\n",
       "  'the',\n",
       "  'reproach',\n",
       "  'of',\n",
       "  'being',\n",
       "  'too',\n",
       "  'amiable',\n",
       "  'edward',\n",
       "  'was',\n",
       "  'admitted',\n",
       "  'to',\n",
       "  'her',\n",
       "  'presence',\n",
       "  'and',\n",
       "  'pronounced',\n",
       "  'to',\n",
       "  'be',\n",
       "  'again',\n",
       "  'her',\n",
       "  'son',\n",
       "  'her',\n",
       "  'family',\n",
       "  'had',\n",
       "  'of',\n",
       "  'late',\n",
       "  'been',\n",
       "  'exceedingly',\n",
       "  'fluctuating',\n",
       "  'for',\n",
       "  'many',\n",
       "  'years',\n",
       "  'of',\n",
       "  'her',\n",
       "  'life',\n",
       "  'she',\n",
       "  'had',\n",
       "  'had',\n",
       "  'two',\n",
       "  'sons',\n",
       "  'but',\n",
       "  'the',\n",
       "  'crime',\n",
       "  'and',\n",
       "  'annihilation',\n",
       "  'of',\n",
       "  'edward',\n",
       "  'a',\n",
       "  'few',\n",
       "  'weeks',\n",
       "  'ago',\n",
       "  'had',\n",
       "  'robbed',\n",
       "  'her',\n",
       "  'of',\n",
       "  'one',\n",
       "  'the',\n",
       "  'similar',\n",
       "  'annihilation',\n",
       "  'of',\n",
       "  'robert',\n",
       "  'had',\n",
       "  'left',\n",
       "  'her',\n",
       "  'for',\n",
       "  'a',\n",
       "  'fortnight',\n",
       "  'without',\n",
       "  'any',\n",
       "  'and',\n",
       "  'now',\n",
       "  'by',\n",
       "  'the',\n",
       "  'resuscitation',\n",
       "  'of',\n",
       "  'edwards',\n",
       "  'he',\n",
       "  'had',\n",
       "  'one',\n",
       "  'again',\n",
       "  'inspite',\n",
       "  'of',\n",
       "  'his',\n",
       "  'being',\n",
       "  'allowed',\n",
       "  'once',\n",
       "  'more',\n",
       "  'to',\n",
       "  'live',\n",
       "  'however',\n",
       "  'he',\n",
       "  'did',\n",
       "  'not',\n",
       "  'feel',\n",
       "  'the',\n",
       "  'continuance',\n",
       "  'of',\n",
       "  'his',\n",
       "  'existence',\n",
       "  'secure',\n",
       "  'till',\n",
       "  'he',\n",
       "  'had',\n",
       "  'revealed',\n",
       "  'his',\n",
       "  'present',\n",
       "  'engagement',\n",
       "  'for',\n",
       "  'the',\n",
       "  'publication',\n",
       "  'of',\n",
       "  'that',\n",
       "  'circumstance',\n",
       "  'he',\n",
       "  'feared',\n",
       "  'might',\n",
       "  'give',\n",
       "  'a',\n",
       "  'sudden',\n",
       "  'turn',\n",
       "  'to',\n",
       "  'his',\n",
       "  'constitution',\n",
       "  'and',\n",
       "  'carry',\n",
       "  'him',\n",
       "  'off',\n",
       "  'as',\n",
       "  'rapidly',\n",
       "  'as',\n",
       "  'before',\n",
       "  'with',\n",
       "  'apprehensive',\n",
       "  'caution',\n",
       "  'therefore',\n",
       "  'it',\n",
       "  'was',\n",
       "  'revealed',\n",
       "  'and',\n",
       "  'he',\n",
       "  'was',\n",
       "  'listened',\n",
       "  'to',\n",
       "  'with',\n",
       "  'unexpected',\n",
       "  'calmness',\n",
       "  'm'],\n",
       " Decimal('3.686929761835359751133620259E-632'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "segmentString(doc[:800])\n",
    "# segmentString('wheninthecourseofhumanevents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a probability dictionary out of our bigram_counts dictionary called `p_bigram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1024908267229 is the number of tokens in the corpus\n",
    "p_bigram = ngram_Prob(bigram_dict, 1024908267229, avoid_long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function `cond_prob` which returns P(word | previous_word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cond_prob(word, prev):\n",
    "    \"The conditional probability P(word | previous-word).\" \n",
    "    try:\n",
    "        return p_bigram[prev + ' ' + word]/float(p_unigram[prev])\n",
    "    # If the bigram is not documented in the top 1/4 million English bigrams, just return unigram(word).\n",
    "    except KeyError:\n",
    "        return p_unigram(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log10\n",
    "def memoize2(function):\n",
    "    memo = {}\n",
    "    def helper(x,y):\n",
    "        if (x,y) not in memo.keys():\n",
    "            memo[(x,y)] = function(x,y)\n",
    "        return memo[(x,y)]\n",
    "    return helper\n",
    "\n",
    "@memoize2\n",
    "def segment2(text, prev='<S>'):\n",
    "    \"Return (log P(words), words), where words is the best segmentation.\"\n",
    "    if not text: return 0.0, []\n",
    "    candidates = [combine(log10(cond_prob(first, prev)), first, segment2(rem, first)) for first,rem in splits(text)]\n",
    "    return max(candidates)\n",
    "                                                            \n",
    "def combine(Pfirst, first, (Prem, rem)):\n",
    "    \"Combine first and rem results into one (probability, words) pair.\"\n",
    "    # We add here because we're dealing with the logs\n",
    "    return Pfirst+Prem, [first]+rem    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-24.959286263365577,\n",
       " ['when',\n",
       "  'in',\n",
       "  'the',\n",
       "  'course',\n",
       "  'of',\n",
       "  'human',\n",
       "  'events',\n",
       "  'it',\n",
       "  'becomes',\n",
       "  'necessary'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment2(\"wheninthecourseofhumaneventsitbecomesnecessary\", '<S>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
