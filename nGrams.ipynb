{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import kenlm\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import util\n",
    "import itertools\n",
    "from helpers import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider a *unigram* model, where the probability of seeing each word is independent of the words before and after it.\n",
    "$$P(W1:n) = Î k=1:nP(Wk)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the file\n",
    "with open('count_unigrams.txt','r') as f:\n",
    "    unigram_counts = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in a word and a count and splits it into the word and the integer.\n",
    "ex) parse_word_count('the\\t23135851162\\n') = (the, 23135851162)\n",
    "'''\n",
    "def parse_word_count(string):\n",
    "    return (string.split(\"\\t\")[0], int(string.split(\"\\t\")[1].split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A dictionary with (word: probability) pairs for 1/3 million most frequent English words \n",
    "unigram_dict = {parse_word_count(line)[0]:parse_word_count(line)[1] for line in unigram_counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a probability distribution dictionary `unigram_probs` from `unigram_dict`. But we will examine many, many \"words\" that are not truly words. Our `unigram_probs` dictionary must return a very small value in that case. (We do not want to simply return 0, because Jane Austen may use her own words (made up words, proper nouns, etc.) that are not in our data set. Instead, following Peter Norvig (CITE), we'll create a class that returns a probability given a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Unigram_Prob(dict):\n",
    "    def __init__(self, data, N, fn):\n",
    "        for key,count in data.iteritems():\n",
    "            self[key] = count\n",
    "        self.N = float(N)\n",
    "        self.fn = fn\n",
    "    def __call__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]/self.N\n",
    "        else: \n",
    "            return self.fn(key, self.N)\n",
    "\n",
    "\n",
    "'''\n",
    "This function returns a probailtiy on unrecognized words. \n",
    "It makes it more unlikely for long unrecognized words to be used than short unrecognized words.\n",
    "The probability is inversely proportional to the length of the unrecognized word.\n",
    "'''\n",
    "def avoid_long_words(word, N):\n",
    "    return 10./(N * 10**len(word))\n",
    "\n",
    "N = 1024908267229 ## Number of tokens in corpus\n",
    "Pw = Unigram_Prob(unigram_dict, N, avoid_long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This memoizing function caches the results of previous calls to the segment\n",
    "function so that each results doesn't have to be recomputed.\n",
    "\n",
    "ex) segment(\"helloworld\") doesn't have to compute segment(\"lloworld\") except for once.\n",
    "\n",
    "The memoizing function helps segment only call itself n times, rather than 2^n times.\n",
    "'''\n",
    "def memoize(function):\n",
    "    memo = {}\n",
    "    def helper(x):\n",
    "        if x not in memo.keys():\n",
    "            memo[x] = function(x)\n",
    "        return memo[x]\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters\n",
    "    Corpus: The flattened text which we need to segment.\n",
    "Output\n",
    "    A list of words, separated according to our probability model.\n",
    "\n",
    "Ex) segment('helloworld') = ['hello','world']\n",
    "'''\n",
    "@memoize\n",
    "def segment(corpus):\n",
    "    if not corpus: \n",
    "        return []\n",
    "    candidates = tuple([first]+segment(remaining) for first,remaining in splits(corpus))\n",
    "    return max(candidates, key=Pwords)\n",
    "\n",
    "def splits(text, L=20):\n",
    "    '''\n",
    "    Return a list of all possible splits of the text, where length(first word)<=L.\n",
    "    '''\n",
    "    return [(text[:i+1], text[i+1:]) for i in range(min(len(text), L))]\n",
    "\n",
    "def Pwords(words):\n",
    "    '''\n",
    "    The Naive Bayes probability of a sequence of words.\n",
    "    '''\n",
    "    return product([uProb(w) for w in words])\n",
    "\n",
    "def uProb(word):\n",
    "    '''\n",
    "    Returns the unigram probability of a word by consulting unigram data.\n",
    "    '''\n",
    "    return Pw.__call__(word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decl = 'wheninthecourseofhumanevents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'in', 'the', 'course', 'of', 'human', 'events']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(decl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
